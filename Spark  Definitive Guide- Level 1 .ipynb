{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6684e73-ec3b-4bdb-ba2b-0f4457939077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Basic Structred Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "576d4f3c-9da3-441e-934d-bdfd049b5cbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SparkTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6850bed-2743-4bd9-a41c-8a8e3770221d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import * #import all types from pyspark\n",
    "from pyspark.sql.functions import * #import all fucntions from pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58860a89-5cb7-4c00-b94f-44e502ce2415",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- IntegerType() - # Python has a lenient definition of “integer.” Numbers that are too large will be rejected by Spark SQL if you use the IntegerType(). It’s best practice to use LongType\\\n",
    "- LongType() - Numbers will be converted to 8-byte signed integer numbers at runtime\\\n",
    "- FloatType() - Numbers will be converted to 4-byte singleprecision floating-point\\ \n",
    "- DecimalType() - Decimal\\\n",
    "- StringType() - string\\\n",
    "- DateType() - datetime.date \n",
    "- StructType(fields) - list or tuple - fields is a list of StructFields. Also, fields StructFields. Also, fields with the same name are not allowed.\\\n",
    "- StructField(name, dataType, [nullable]) - The default value of nullable is True. The value type in Python of the data type of this field (for example, Int for a StructField with the data type IntegerType)\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "571c4398-52dc-411c-8a66-8123e1b3f74c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Frame & Schema "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5964b9bc-189b-413d-ae59-a307c6b6bff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **Dataframe**- Consists of Series of record- Rows  (Rows represent individual records ) & Coloumns (store values for each record and can contain computed expressions (operations on the data).) \\\n",
    "**DataFrame** is a structured collection of data stored in a distributed manner across a cluster. It is built on top of Spark’s Dataset API. Every row is an object of Row type in Spark.Every column represents either a value or a computed expression (like transformations).\\\n",
    "\n",
    "**What is a Schema**\\\n",
    "A **schema** defines the structure of the DataFrame, including:\\\n",
    "- Column names (e.g., \"ID\", \"Name\", \"Age\").\\\n",
    "- Column data types (e.g., \"Integer\", \"String\", \"Float\").\\\n",
    "> Without a schema, Spark will infer the types automatically, which may not always be accurate.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1a325b4-fb2e-4133-94aa-428681ce84b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### What is Partitioning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c59ead0-e915-4fa1-95c1-d5673671f6e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "Partitioning = Dividing Data for Efficient Processing\\\n",
    "A DataFrame spreads its data across multiple machines (nodes) in a distributed cluster.\\\n",
    "Each machine processes a portion of the data instead of loading everything into a single node.\\\n",
    "This helps speed up query execution and optimize memory usage.\n",
    "**Types of Partitioning in Spark**\\\n",
    "There are two types of partitioning in a DataFrame:\\\n",
    "**1. Column-Based Partitioning (Deterministic Partitioning)**\n",
    "  - Data is split based on values in a specific column.\n",
    "  - Helps when querying specific column values because Spark only scans relevant partitions\\\n",
    "  - Example: Partition by \"Age\" Column\n",
    "If we partition the DataFrame by Age, Spark stores:\n",
    "  - Partition 1: All records where Age is between 20-30\n",
    "  - Partition 2: All records where Age is between 30-40\n",
    "  - Partition 3: All records where Age is above 40\n",
    "\n",
    "**2. Random Partitioning (Non-Deterministic Partitioning)**\n",
    "  - Data is randomly split across multiple partitions.\n",
    "  - This is useful when there is no specific column to partition by.\n",
    "  - Ensures load balancing across the cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92734bf5-20cb-4b9b-b9ba-20f3b36ad9c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Read Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "327191de-6100-4a82-b72e-3a9eddc60173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: [FileInfo(path='dbfs:/FileStore/tables/2015_summary.csv', name='2015_summary.csv', size=7080, modificationTime=1741666214000),\n FileInfo(path='dbfs:/FileStore/tables/2015_summary.json', name='2015_summary.json', size=21368, modificationTime=1741692430000),\n FileInfo(path='dbfs:/FileStore/tables/BigMart_Sales-1.csv', name='BigMart_Sales-1.csv', size=869537, modificationTime=1740727435000),\n FileInfo(path='dbfs:/FileStore/tables/BigMart_Sales.csv', name='BigMart_Sales.csv', size=869537, modificationTime=1740138394000),\n FileInfo(path='dbfs:/FileStore/tables/drivers.json', name='drivers.json', size=180812, modificationTime=1740139360000)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls('FileStore/tables') # function to fidn the Filepath and Filestore in the dbfs filestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03e9e858-1d48-4aff-83ad-97e9ebe7ef6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.format('json').load('dbfs:/FileStore/tables/2015_summary.json')\n",
    "#read json file. Spark tills input format('Json')- is Json file. \n",
    "# #load ('filepath') loads the data from the given file path.\n",
    "# df is our DataFrame, which contains flight data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2405625-6a99-4813-823c-e62d4c91633e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "\n",
    "# Since we haven’t defined a schema, Spark automatically infers the schema based on the data in the JSON file.\n",
    "# Spark infers the schema from the data (this is called schema-on-read)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d8ea1f5-adb7-4b07-94d4-0a4f1a1ef338",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True)])"
     ]
    }
   ],
   "source": [
    "df.schema\n",
    "# use to give the output schema which is of Structype and Stuctfield defined. OBSERVE () NOT used in SCHEMA. AS IT PROPERTY not a Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "206e6ec8-c04b-44b0-8969-71e589f3b21d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Manually Defining a Schema\n",
    "In production (ETL pipelines), we manually define schemas to:\\\n",
    "✅ Ensure correct data types (avoid wrong inferences).\\\n",
    "✅ Improve performance (avoid slow auto-detection).\\\n",
    "✅ Handle missing data properly (decide if NULLs are allowed)\n",
    "\n",
    "StructType () and StructField () to manually define a schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b74f8f3e-1ec4-4585-be62-38c80e88cc16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "my_schema = StructType(\n",
    "[\n",
    "StructField( 'DEST_COUNTRY_NAME', StringType(), True ), # Allows NULL values\n",
    "StructField('ORIGIN_COUNTRY_NAME', StringType(), True),  # Allows NULL values\n",
    "StructField('count', LongType(), False) # doesnt Allows NULL valu\n",
    "]\n",
    "\n",
    ")\n",
    "\n",
    "#StructType([]): Represents the entire schema (like a table structure).\n",
    "# StructField(name, data_type, nullable): Defines each column’s name, type, and NULL handling.\n",
    "# eg: 'DEST_COUNTRY_NAME', StringType(), True → A column of type String, allowing NULL values.\n",
    "# \"count\", LongType(), False → A column of type Long, NOT allowing NULL values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f21209d8-b9ac-401e-a46f-7092cfc13de2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Loading JSON with a Defined Schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a9a626a-e29b-499a-8358-f91f5bfd077b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.format('json').schema(my_schema)\\\n",
    "    .load('dbfs:/FileStore/tables/2015_summary.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff52d1a5-e2dd-4d16-bd29-5f16d58f0f7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "971a2c03-878b-48f2-afb3-1146e3921d36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Adding Metadata to the schema\n",
    "Metadata in PySpark is additional information stored in the schema of a DataFrame. It does not affect how Spark reads or processes data but can be useful for documentation, optimizations, or machine learning tasks. Metadata is extra information about a column in a DataFrame schema. It does not change the actual data or column structure but helps store additional details eg : Column descriptions, Data source information, Constraints etc.\\\n",
    "\n",
    "PySpark stores metadata using a dictionary (key-value pairs) inside the StructField of a StructType\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88a4ae56-38d7-4e35-9d55-f96daeac2722",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Defining a schema with metadata\n",
    "mySchema_new = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType(), True, metadata={\"description\": \"Destination Country\"}),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True, metadata={\"description\": \"Origin Country\"}),\n",
    "    StructField(\"count\", LongType(), False, metadata={\"hello\": \"world\", \"data_source\": \"Bureau of Transport Statistics\"})\n",
    "])\n",
    "#We add metadata when defining a schema manually using the metadata parameter in the StructField constructor.\n",
    "# metadata={\"description\": \"Destination Country\"} → This adds a description to the DEST_COUNTRY_NAME column.\n",
    "\n",
    "# Applying the schema while reading a JSON file\n",
    "df_new = spark.read.format(\"json\").schema(mySchema_new).load(\"dbfs:/FileStore/tables/2015_summary.json\")\n",
    "\n",
    "# Printing schema with metadata\n",
    "df_new.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1192fe23-cdfa-4066-ba73-dab7fd13f936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Access Metadata\n",
    "To retrieve metadata from a column, we use  .metadata on the required coloumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6d6f6ed-33aa-4222-9516-48c8a50391ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[30]: {'data_source': 'Bureau of Transport Statistics', 'hello': 'world'}"
     ]
    }
   ],
   "source": [
    "df_new.schema[\"count\"].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78b01891-7c4f-463e-aa3a-4a6b0435a760",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Columns and Expressions in PySpark\n",
    "In PySpark, a column is a logical representation of a value that is calculated for each row in a DataFrame.\\\n",
    "Columns are defined using functions like col() or column(). mostly col( ) is used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d15e0fc4-51c5-4b98-9ad4-17e81442087a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1002669071205069>:2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# to refer tot he coloum we can use\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcount\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# to refer to the coloumn in the specific datafarame we can use\u001B[39;00m\n",
       "\u001B[1;32m      4\u001B[0m df\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcount\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'col' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-1002669071205069>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# to refer tot he coloum we can use\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcount\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# to refer to the coloumn in the specific datafarame we can use\u001B[39;00m\n\u001B[1;32m      4\u001B[0m df\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcount\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\n\u001B[0;31mNameError\u001B[0m: name 'col' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'col' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to refer to the coloum we can use in the expressions\n",
    "col('count')\n",
    "# to refer to the coloumn in the specific datafarame we can use in the expressions when working with multiple dataframes\n",
    "df.col('count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d40673c-aca4-4894-b3ec-70c95d5add2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[41]: ['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
     ]
    }
   ],
   "source": [
    "df.columns\n",
    "\n",
    "# gives list of all coloumn in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d35867a-f70b-4bd1-afa0-c146a95a7435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Expressions in PySpark?\n",
    "- Expressions take column names as input.\n",
    "- They process values in each row based on the transformation.\n",
    "- They return a single calculated value per row\n",
    "\n",
    "expr() - Fucntion is used to create a expression on the coloum. It helps in performing column transformations, calculations, and conditions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15665b82-ee70-4915-9b6c-5ff5be1dd320",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>salary</th><th>new_salary</th></tr></thead><tbody><tr><td>1</td><td>100</td><td>110.0</td></tr><tr><td>2</td><td>200</td><td>220.0</td></tr><tr><td>3</td><td>300</td><td>330.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         100,
         "110.0"
        ],
        [
         2,
         200,
         "220.0"
        ],
        [
         3,
         300,
         "330.0"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "new_salary",
         "type": "\"decimal(23,1)\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [(1, 100), (2, 200), (3, 300)]\n",
    "df_dummy = spark.createDataFrame(data, [\"id\", \"salary\"])\n",
    "\n",
    "# Using expr() to increase salary by 10%\n",
    "df_expression = df_dummy.withColumn(\"new_salary\", expr(\"salary * 1.1\"))\n",
    "df_expression.display()\n",
    "\n",
    "# withcoloum fucntion is sused to add coloum to the dataframe base don the expression. discussed later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "374cf40f-9888-4f19-b15a-2390f104bd1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### expr(\"someCol - 5\") is the same transformation as performing col(\"someCol\") - 5, or even expr(\"someCol\") - 5. That’s because Spark compiles these to a logical tree specifying the order of operations.\n",
    "\n",
    "because:\n",
    "- Columns are just expressions.\n",
    "- Columns and transformations of those columns compile to the same logical plan as parsed expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cd7f554-7cb8-41fb-98d9-0b210b5c67f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>salary</th><th>new_salary</th></tr></thead><tbody><tr><td>1</td><td>100</td><td>110.00000000000001</td></tr><tr><td>2</td><td>200</td><td>220.00000000000003</td></tr><tr><td>3</td><td>300</td><td>330.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         100,
         110.00000000000001
        ],
        [
         2,
         200,
         220.00000000000003
        ],
        [
         3,
         300,
         330.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "new_salary",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_expression = df_dummy.withColumn(\"new_salary\", expr(\"salary\") * 1.1)\n",
    "\n",
    "df_expression.display()\n",
    "\n",
    "# expr(\"salary * 1.1\") and  expr(\"salary\") * 1.1 -- both gave the same result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5304fb97-b01e-43a1-a2cb-8dc86ba91ca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Records and Rows in Spark\n",
    "In Apache Spark, a row in a DataFrame represents a single record of data. Each row contains multiple values, one for each column in the DataFrame. Spark stores these rows as Row objects, which are used to manipulate and process data.\n",
    "Spark stores Row objects internally as arrays of bytes to optimize performance. However, you don’t need to worry about this because Spark provides column expressions to manipulate data, meaning you don’t directly handle byte arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d963a99-b7f2-4970-9714-e59114233d62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[48]: Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"
     ]
    }
   ],
   "source": [
    "df.first()\n",
    "# This command returns a Row object, which contains the values of all columns for the first record in the DataFrame.\n",
    "#Each value corresponds to a column in the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba40fa65-96de-4b2e-9766-b80ac6254c87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating Rows Manually\n",
    "\n",
    "Row() - Function is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b72cacd0-4062-40cf-a4da-7dfa615ed33c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[80]: 'Hello'"
     ]
    }
   ],
   "source": [
    "myRow = Row(\"Hello\", None, 1, False)\n",
    "\n",
    "# Accessing Data in Rows\n",
    "myRow[0]\n",
    "# Once you have a Row object, you can access its values by position (indexing), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84c1c890-4d5c-4d92-a48e-8096dc9c8f73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[64]: 1"
     ]
    }
   ],
   "source": [
    "myRow[2] # Once you have a Row object, you can access its values by position (indexing), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff300f32-3545-448f-9502-2844e4450b35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[78]: Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"
     ]
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19f1ad97-34b5-43ab-bbee-81fcae31854d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[70]: 'India'"
     ]
    }
   ],
   "source": [
    "df.collect()[4][1]\n",
    "\n",
    "# Using collect() .collect() converts the entire DataFrame into a list of Row objects.\n",
    "# [4] selects the 5th row (indexing starts from 0).\n",
    "# [1] selects the 2nd column from that row (indexing starts from 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52031bed-9e14-42f6-9b88-810f8863da8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[87]: Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)"
     ]
    }
   ],
   "source": [
    "df.take(5)[4]\n",
    "\n",
    "#this gives a list of rows based on the argument\n",
    "# The .take(n) function in PySpark is used to retrieve the first n rows from a DataFrame and return them as a list of Row objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e4e26a4-4cfc-4a49-8ed0-ee68e5dbf75b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[88]: Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)"
     ]
    }
   ],
   "source": [
    "df.collect()[4]\n",
    "\n",
    "# .collect(), but instead of bringing all data to the driver, it only fetches the first n rows efficiently.\n",
    "# OBSERVE this give a Row object based on the index given here all rows are called in a a array and then indexed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30c1f0fc-b1d6-4d03-b93d-d251cf142413",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Data Frame Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6d40d23-872f-4748-8125-7ff1f7de878f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### select and selectExpr\n",
    "When working with PySpark DataFrames, you often need to select specific columns, modify their values, or create new columns. Two important methods to achieve this are:\n",
    "\n",
    "- **select()** - Used to select specific columns from a DataFrame.\n",
    "- **selectExpr()** - Used to write SQL-like expressions directly within the selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "796bf639-8025-4490-966a-bda9f840974c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n|DEST_COUNTRY_NAME|\n+-----------------+\n|    United States|\n|    United States|\n|    United States|\n|            Egypt|\n|    United States|\n+-----------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.select('DEST_COUNTRY_NAME').show(5)\n",
    "#You can select one column from the DataFrame by passing its name as a string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4737b0c1-4806-4e2a-8dfe-d3a4e4d34b85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n+-----------------+-------------------+\n|    United States|            Romania|\n|    United States|            Croatia|\n|    United States|            Ireland|\n|            Egypt|      United States|\n|    United States|              India|\n+-----------------+-------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(5)\n",
    "#To select multiple columns, just pass multiple column names as strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59a1798e-7c1b-4870-bca5-caf6af6041e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+\n|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n+-----------------+-----------------+-----------------+\n|    United States|    United States|    United States|\n|    United States|    United States|    United States|\n+-----------------+-----------------+-----------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, column, expr\n",
    "\n",
    "df.select(\n",
    "    expr(\"DEST_COUNTRY_NAME\"),  # Using expr\n",
    "    col(\"DEST_COUNTRY_NAME\"),   # Using col()\n",
    "    column(\"DEST_COUNTRY_NAME\") # Using column()\n",
    ").show(2)\n",
    "# PySpark provides multiple ways to reference a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5d58eaa-6f41-41dd-8fd8-faed7a49d138",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n+-----------------+-------------------+\n|    United States|            Romania|\n|    United States|            Croatia|\n+-----------------+-------------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# If you mix column objects (col(\"column_name\")) and strings (\"column_name\") in select(), you’ll get an error.\n",
    " # df.select(col(\"DEST_COUNTRY_NAME\"), \"DEST_COUNTRY_NAME\")  -  - - - -  this code Causes an error!\n",
    "\n",
    "df.select(col(\"DEST_COUNTRY_NAME\"), col(\"ORIGIN_COUNTRY_NAME\")).show(2)  # doesnt produce any error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6af3e051-0825-4712-80ab-44c6df199c5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Selectexpr()\n",
    "It allows SQL-like expressions inside select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e75c99de-bbf5-43c4-8e93-e73790d77a81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n|newColumnName|DEST_COUNTRY_NAME|\n+-------------+-----------------+\n|United States|    United States|\n|United States|    United States|\n+-------------+-----------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Renaming Columns Using selectExpr()\n",
    "\n",
    "df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d102b14-6917-44fc-b5c6-c84fbda8ed68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n+-----------------+-------------------+-----+-------------+\n|    United States|            Romania|   15|        false|\n|    United States|            Croatia|    1|        false|\n|    United States|            Ireland|  344|        false|\n|            Egypt|      United States|   15|        false|\n|    United States|              India|   62|        false|\n+-----------------+-------------------+-----+-------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "#You can use selectExpr() to create a new column using an expression.\n",
    "\n",
    "df.selectExpr(\n",
    "    \"*\",  # Select all original columns\n",
    "    \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\" # Boolean column\n",
    ").show(5)\n",
    "# SELECT *, (DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry FROM dfTable LIMIT 2;- SQL Equivalent of ABove Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "680d41a6-f732-40d7-93cd-4f3e475486c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+\n| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n+-----------+---------------------------------+\n|1770.765625|                              132|\n+-----------+---------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#You can also apply aggregate functions like avg() and count(distinct) in selectExpr().\n",
    "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)\n",
    "\n",
    "\n",
    "# SELECT avg(count), count(distinct(DEST_COUNTRY_NAME)) FROM dfTable LIMIT 2;  # SQL- Equivalent of above code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fdfcb4e-2e3b-4e5e-a443-554310f5e985",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Understanding Spark Literals, Column Manipulation, and Reserved Characters in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4b0858f-443d-453b-a916-4a38cd141397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Converting to Spark Types (Literals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64104592-4ae9-4df1-ba83-03b01b83ba18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "When working with Spark DataFrames, we sometimes need to insert constant values into our dataset. These constant values are called literals in Spark.\\\n",
    "**lit( ) Function** - is used to insert a constant value as a new column or within an expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b7a9c0b-1d0c-4190-b49b-6f38b5393816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n|    United States|          Singapore|    1|\n|    United States|            Grenada|   62|\n|       Costa Rica|      United States|  588|\n|          Senegal|      United States|   40|\n|          Moldova|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.select('*').show(10)\n",
    "# * wildcharter is used to select all coloumns from the data frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df221934-dbda-42eb-b068-dd558ad90e3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Alias Function - alias() function in PySpark is used to rename a column temporarily in a DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a74151f-740b-4341-85fd-0db0a4eca007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n|     DEST_CON|\n+-------------+\n|United States|\n|United States|\n|United States|\n+-------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"DEST_COUNTRY_NAME\").alias(\"DEST_CON\")).show(3)\n",
    "\n",
    "#alias function is applied on the selected coloumn to renmae the coloumn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38326a0b-304e-4e01-845e-66fc7cd6f809",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|Constanat|\n+-----------------+-------------------+-----+---------+\n|    United States|            Romania|   15|      New|\n|    United States|            Croatia|    1|      New|\n+-----------------+-------------------+-----+---------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.select('*', lit('New').alias('Constanat')).show(2)\n",
    "\n",
    "#lit fucntion entered as an argument enter the constant value 'New' and an alias fucntion is used to create a name for the coloumn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c796fb62-d74f-40d8-9e4c-51e733566b95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Adding Coloumns\n",
    "new column can be added using the withColumn() method in PySpark- The withColumn() method is used to create a new column or modify an existing column\\\n",
    "It takes two parameters:\n",
    "- Column name (string)\n",
    "- Expression (which can be a constant, an existing column, or a calculated value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "619bb951-b196-4f96-9167-5f3eef7aa10c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n+-----------------+-------------------+-----+---------+\n|    United States|            Romania|   15|        1|\n|    United States|            Croatia|    1|        1|\n+-----------------+-------------------+-----+---------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"numberOne\", lit(1)).show(2)\n",
    "\n",
    "# here a number one coloum name is added to the data frame and Constatn lit () fucntion is used to fill the coloumns with constatn 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "590d99dd-1d8d-4651-b18b-3c727f2e2fd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n+-----------------+-------------------+-----+-------------+\n|    United States|            Romania|   15|        false|\n|    United States|            Croatia|    1|        false|\n+-----------------+-------------------+-----+-------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\")).show(2)\n",
    "# this use an Exprssion fucntion to identify the  ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME and resturn a boolean value with a new coloumn withincountry attached to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16204dce-dc00-43df-9d0f-6e611a709eac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Renaming Columns\n",
    "The withColumnRenamed() method is used to rename an existing column\\\n",
    "It takes two parameters:\n",
    "- Old column name (string)\n",
    "- New column name (string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc8c1a29-7ce8-425c-9684-323a2cb4222f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[31]: ['dest_coury', 'ORIGIN_COUNTRY_NAME', 'count']"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest_coury\").columns\n",
    "# the output shows DEST_COUNTRY_NAME\" is renamed with \"dest - Coloumn name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1de1c58d-536b-4579-b854-d5566b036d7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Case Sensitivity in Spark- By default, Spark SQL is case insensitive, meaning that \"columnName\" and \"COLUMNNAME\" are treated as the same- But case sensistivity can be kept to true.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d57a6f4-5887-4f8c-993c-b425389c724d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n|ORIGIN_CouNTRY_NAME|\n+-------------------+\n|            Romania|\n|            Croatia|\n|            Ireland|\n|      United States|\n|              India|\n+-------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.select('ORIGIN_CouNTRY_NAME').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53857549-2c63-4ea1-9bb9-ffa158eaa4a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.caseSensitive\", True)\n",
    "# case senitivity is set to True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d14b35a9-5ee9-44a9-9ee6-005e75479470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3893457758963658>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mORIGIN_CouNTRY_NAME\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m5\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n",
       "\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   2980\u001B[0m \n",
       "\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n",
       "\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `ORIGIN_CouNTRY_NAME` cannot be resolved. Did you mean one of the following? [`ORIGIN_COUNTRY_NAME`, `DEST_COUNTRY_NAME`, `count`].;\n",
       "'Project ['ORIGIN_CouNTRY_NAME]\n",
       "+- Relation [DEST_COUNTRY_NAME#11,ORIGIN_COUNTRY_NAME#12,count#13L] json\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-3893457758963658>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mORIGIN_CouNTRY_NAME\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m5\u001B[39m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   2980\u001B[0m \n\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `ORIGIN_CouNTRY_NAME` cannot be resolved. Did you mean one of the following? [`ORIGIN_COUNTRY_NAME`, `DEST_COUNTRY_NAME`, `count`].;\n'Project ['ORIGIN_CouNTRY_NAME]\n+- Relation [DEST_COUNTRY_NAME#11,ORIGIN_COUNTRY_NAME#12,count#13L] json\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `ORIGIN_CouNTRY_NAME` cannot be resolved. Did you mean one of the following? [`ORIGIN_COUNTRY_NAME`, `DEST_COUNTRY_NAME`, `count`].;\n'Project ['ORIGIN_CouNTRY_NAME]\n+- Relation [DEST_COUNTRY_NAME#11,ORIGIN_COUNTRY_NAME#12,count#13L] json\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.select('ORIGIN_CouNTRY_NAME').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a46237b2-cdb7-40e1-9b5a-78ebe69be6e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.caseSensitive\", False)\n",
    "# case senitivity is set to False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "401135d2-37cb-4cf5-92cc-67c37ac3d6a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Removing Columns\n",
    "When working with DataFrames in Apache Spark (PySpark), sometimes we need to remove specific columns that are not required for analysis\n",
    "- The drop( ) function is specifically built for removing columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a97324f2-c057-40ea-9279-31b90287f3cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n|DEST_COUNTRY_NAME|count|\n+-----------------+-----+\n|    United States|   15|\n|    United States|    1|\n+-----------------+-----+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.drop(\"ORIGIN_COUNTRY_NAME\").show(2)\n",
    "# here specificallly the coloumn which is not rewuired is drop from the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "898fc283-fc96-4047-975c-37c27a4c3641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n|count|\n+-----+\n|   15|\n|    1|\n+-----+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# To remove multiple columns, we can pass multiple column names:\n",
    "df.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce2c952d-851a-4225-836b-8414bece9b26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Changing a Column’s Type (cast)\n",
    "Sometimes, we need to convert the data type of a column. This is called casting. if a column is stored as StringType but we need it as IntegerType, we use the cast function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eb1e455-8f48-429e-9195-3a1e96001ccb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# If the count column is currently stored as an integer, but we need it as a long, we can do\n",
    "\n",
    "df.withColumn('count', col('count').cast('long')).printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af61e75f-9054-4bed-96f8-0bff27532eca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Filtering Rows\n",
    "Filtering is used to select only specific rows based on conditions.\n",
    "- filter()\n",
    "- where() (works the same as filter(), but is more similar to SQL syntax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29262558-e459-4b0e-8f60-604aecada774",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n|    United States|            Grenada|   62|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Filtering rows where count < 2\n",
    "df.filter(col('count')>2).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84ead8fd-2f6e-4c75-a33e-79f05345d2da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n|    United States|            Grenada|   62|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# using where fucntion\n",
    "df.where('count>2').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94f856ea-88ca-4ac6-9c4a-e73403d968e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|       United States|          Singapore|    1|\n|             Moldova|      United States|    1|\n|               Malta|      United States|    1|\n|       United States|          Gibraltar|    1|\n|Saint Vincent and...|      United States|    1|\n+--------------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Filtering rows where count < 2 and ORIGIN_COUNTRY_NAME is not \"Croatia\"- Filtering with Multiple Conditions\n",
    "\n",
    "df.where(col('count')<2).where(col('ORIGIN_COUNTRY_NAME') !='Croatia').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "370159ca-5f4e-4141-87a0-365bcd05e25b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|       United States|          Singapore|    1|\n|             Moldova|      United States|    1|\n|               Malta|      United States|    1|\n|       United States|          Gibraltar|    1|\n|Saint Vincent and...|      United States|    1|\n+--------------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Filtering with Multiple Conditions - With Filter Fucntion\n",
    "df.filter ((col('count')<2) &( col('ORIGIN_COUNTRY_NAME') != 'Croatia' )   ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e27c49b4-c986-4f01-8fc9-430cc63ddaed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### getting Distint rows\n",
    "operation in DataFrames is finding distinct (unique) values in one or more columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5075f0bd-33de-4d08-8d25-d5fe4dc2d9ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n|ORIGIN_COUNTRY_NAME|\n+-------------------+\n|          Singapore|\n|      United States|\n|              India|\n|            Croatia|\n|            Ireland|\n+-------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().show(5)\n",
    "# select the coloumn  ORIGIN_COUNTRY_NAME and display disitinct values in the coloumn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e2c9adc-335f-42b4-a4cf-3499a7b5c078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[70]: 125"
     ]
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()\n",
    "# select the coloumn  ORIGIN_COUNTRY_NAME and display disitinct values in the coloumn and count fucntion counts the total distinct value in the coloum\n",
    "\n",
    "# SELECT COUNT(DISTINCT(ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME)) FROM dfTable;- SQL equivalent of the above code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c7ddffc-8c03-467b-a43d-c6ca098c731c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Understanding Random Splits, Union, Sorting, and Limit in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c321829-ff55-422a-868f-b3f5f144b0db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Random Splits (randomSplit())**\n",
    "\n",
    "When working with machine learning models, we often need to split our dataset into multiple subsets, such as:\n",
    "\n",
    "- Training Set (used to train the model)\n",
    "- Validation Set (used to fine-tune the model)\n",
    "- Test Set (used to evaluate model performance)\n",
    "PySpark allows us to randomly split a DataFrame into multiple parts using the .randomSplit() function.\n",
    "\n",
    "- You define a list of proportions that determine the size of each subset.\n",
    "- You can also set a seed to ensure the randomness is reproducible (i.e., get the same split every time).generally 42 is used but you can use any integer value as a seed. If you use the same seed every time, you will get the same split \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58996408-e559-4762-a63c-112087ceefef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[82]: 63"
     ]
    }
   ],
   "source": [
    "dataFrames = df.randomSplit([0.25, 0.75], seed=42)\n",
    "#[0.25, 0.75] → The first DataFrame gets approximately 25% of the data, and the second gets 75%.\n",
    "#seed=42 → Ensures that every time you run this, the split remains the same.\n",
    "dataFrames[0].count() \n",
    "# The result is a list of DataFrames, where dataFrames[0] is the first split and dataFrames[1] is the second split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cf2a74b-f82e-4dd8-bdff-8cf0ba319e0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|          Austria|      United States|   62|\n|           Brazil|      United States|  853|\n|         Bulgaria|      United States|    3|\n|     Cook Islands|      United States|   13|\n|       Costa Rica|      United States|  588|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "dataFrames[0].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "780416cb-6d86-4323-9f19-6c193ee244e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-----+\n|  DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-------------------+-------------------+-----+\n|            Algeria|      United States|    4|\n|             Angola|      United States|   15|\n|           Anguilla|      United States|   41|\n|Antigua and Barbuda|      United States|  126|\n|          Argentina|      United States|  180|\n+-------------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "dataFrames[1].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "741ff233-b618-4c30-99ef-a4a1eb274063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Concatenating and Appending Rows (union())\n",
    "DataFrames are immutable – you cannot modify them in place. Instead, if you want to add new rows, you must use the .union() function\\\n",
    "in union() function\n",
    "\n",
    "- You combine two DataFrames into one.\n",
    "- Both DataFrames must have the same schema (same columns and data types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "582351c9-35fd-41c3-a8b3-830aa28d1621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|      New Country|      Other Country|    5|\n+-----------------+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Appending new rows to a DataFrame\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create new DataFrame with the same schema\n",
    "newDF = spark.createDataFrame([\n",
    "    (\"New Country\", \"Other Country\", 5),\n",
    "    (\"New Country 2\", \"Other Country 3\", 1)\n",
    "], df.schema)\n",
    "\n",
    "# Union and filter in one step\n",
    "df.union(newDF).filter(col('DEST_COUNTRY_NAME') == 'New Country').show(150)\n",
    "\n",
    "# PySpark's createDataFrame() method is used\n",
    "    #This function is used to create a DataFrame in PySpark.\n",
    "    #data: The first argument is a list of tuples representing the rows.\n",
    "    #schema: The second argument is the schema (column structure) of the DataFrame. Instead of manually defining column names and data types, we copy the schema from an existing DataFrame \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bbf4c2d-9cbe-49f0-ab2a-5a9d0bcad641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### unionByName() function -  is used to combine two DataFrames by matching column names, even if the columns are in a different order or missing in one of the DataFrames.\n",
    "- df1.unionByName(df2, allowMissingColumns=False)- allowMissingColumns=False (default): Throws an error if a column is missing in one DataFrame\n",
    "- allowMissingColumns=True: Allows combining even if one DataFrame has additional columns\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b930a15-5eee-49ab-921e-da9132c16f93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n|   Name|Age|\n+-------+---+\n|  Alice| 25|\n|    Bob| 30|\n|Charlie| 40|\n|  David| 50|\n+-------+---+\n\n"
     ]
    }
   ],
   "source": [
    "data1 = [(\"Alice\", 25), (\"Bob\", 30)]\n",
    "columns1 = [\"Name\", \"Age\"]\n",
    "\n",
    "data2 = [(40, \"Charlie\"), (50, \"David\")]\n",
    "columns2 = [\"Age\", \"Name\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, columns1)\n",
    "df2 = spark.createDataFrame(data2, columns2)\n",
    "\n",
    "# UnionByName\n",
    "df_union = df1.unionByName(df2)\n",
    "df_union.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e667c20-a6a4-4778-9cdc-9c67367c6e5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------+\n|   Name|Age|Country|\n+-------+---+-------+\n|  Alice| 25|   null|\n|    Bob| 30|   null|\n|Charlie| 40|    USA|\n|  David| 50| Canada|\n+-------+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Handling Missing Columns\n",
    "data1 = [(\"Alice\", 25), (\"Bob\", 30)]\n",
    "columns1 = [\"Name\", \"Age\"]\n",
    "\n",
    "data2 = [(40, \"Charlie\", \"USA\"), (50, \"David\", \"Canada\")]\n",
    "columns2 = [\"Age\", \"Name\", \"Country\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, columns1)\n",
    "df2 = spark.createDataFrame(data2, columns2)\n",
    "\n",
    "# Using allowMissingColumns=True\n",
    "df_union = df1.unionByName(df2, allowMissingColumns=True)\n",
    "df_union.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2aff9d08-dcd2-4498-bc80-481277944dd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Sorting Rows (sort() and orderBy())\n",
    "Sorting is used to arrange data in a specific order\n",
    "- Using .sort() - Sorts the DataFrame in ascending order by default\n",
    "- Using .orderBy() -  Works the same way as .sort(), but allows sorting by multiple columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da3dc34b-4ed8-459d-8f2c-af3a3c92ac5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|               Malta|      United States|    1|\n|Saint Vincent and...|      United States|    1|\n|       United States|            Croatia|    1|\n|       United States|          Gibraltar|    1|\n|       United States|          Singapore|    1|\n+--------------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"count\").show(5)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "786e6d4c-5ff4-4d85-bfe1-9727ce145343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|     Burkina Faso|      United States|    1|\n|    Cote d'Ivoire|      United States|    1|\n|           Cyprus|      United States|    1|\n|         Djibouti|      United States|    1|\n|        Indonesia|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)\n",
    "\n",
    "#First, sort by count (ascending by default).\n",
    "#Then, sort by DEST_COUNTRY_NAME (if two rows have the same count value).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36bbd5b9-366e-45c0-b63b-a5fd63c720cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n+-----------------+-------------------+------+\n|    United States|      United States|370002|\n|    United States|             Canada|  8483|\n|           Canada|      United States|  8399|\n|    United States|             Mexico|  7187|\n|           Mexico|      United States|  7140|\n+-----------------+-------------------+------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# If you want to sort in descending order, use the desc() function\n",
    "df.sort(desc(\"count\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f036bb96-5bd2-43dc-90cb-a46651501dca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n+-----------------+-------------------+------+\n|    United States|      United States|370002|\n|    United States|             Canada|  8483|\n|           Canada|      United States|  8399|\n|    United States|             Mexico|  7187|\n|           Mexico|      United States|  7140|\n+-----------------+-------------------+------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(desc(\"count\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "074b39e2-3092-40fa-8d46-10876cec8fd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n+-----------------+-------------------+------+\n|    United States|      United States|370002|\n|    United States|             Canada|  8483|\n|           Canada|      United States|  8399|\n|    United States|             Mexico|  7187|\n|           Mexico|      United States|  7140|\n+-----------------+-------------------+------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# sorting multiple coloumns\n",
    "df.orderBy(desc(\"count\"), asc(\"DEST_COUNTRY_NAME\")).show(5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4533e19d-6b23-4113-b77c-b26a96a12f06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Handling NULL Values in Sorting\n",
    "If your DataFrame contains NULL values, you can control where they appear in sorting:\n",
    "\n",
    "- **asc_nulls_first** → NULLs come first when sorting in ascending order.\n",
    "- **asc_nulls_last** → NULLs come last when sorting in ascending order.\n",
    "- **desc_nulls_first** → NULLs come first when sorting in descending order.\n",
    "- **desc_nulls_last** → NULLs come last when sorting in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac139a29-48e1-4745-9f65-22876bf85be7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n+-----------------+-------------------+------+\n|    United States|      United States|370002|\n|    United States|             Canada|  8483|\n|           Canada|      United States|  8399|\n|    United States|             Mexico|  7187|\n|           Mexico|      United States|  7140|\n+-----------------+-------------------+------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(col(\"count\").desc_nulls_last()).show(5)\n",
    "\n",
    "#This ensures NULL values appear at the bottom of the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f6bbfbf-8338-40fd-b589-292923b3f438",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|       United States|            Croatia|    1|\n|       United States|          Singapore|    1|\n|             Moldova|      United States|    1|\n|               Malta|      United States|    1|\n|       United States|          Gibraltar|    1|\n|Saint Vincent and...|      United States|    1|\n|            Suriname|      United States|    1|\n|       United States|             Cyprus|    1|\n|        Burkina Faso|      United States|    1|\n|            Djibouti|      United States|    1|\n|       United States|            Estonia|    1|\n|              Zambia|      United States|    1|\n|              Cyprus|      United States|    1|\n|       United States|          Lithuania|    1|\n|       United States|           Bulgaria|    1|\n|       United States|            Georgia|    1|\n|       United States|            Bahrain|    1|\n|       Cote d'Ivoire|      United States|    1|\n|       United States|   Papua New Guinea|    1|\n|              Kosovo|      United States|    1|\n+--------------------+-------------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.sortWithinPartitions(\"count\").show(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd4f06dd-0bdd-4a50-b28b-dd90ada28d2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- sort() / orderBy() → Global sorting - Sorts the entire DataFrame across all partitions. Requires a full shuffle of data, which is costly. Performance hit due to shuffling \n",
    "- sortWithinPartitions() → Sorting within each partition- Only sorts data within individual partitions without reshuffling between partitions.Faster than global sorting. No full shuffle, making it more efficient for operations that don’t require full sorting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1c9cf17-c08d-4b8f-9e0c-09afaf3ed19a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|            India|                USA|    8|\n|              USA|              India|   10|\n|           Canada|                USA|   15|\n|           France|            Germany|    5|\n|          Germany|             France|    6|\n|              USA|             Canada|   12|\n+-----------------+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample Data\n",
    "data = [\n",
    "    (\"USA\", \"India\", 10),\n",
    "    (\"France\", \"Germany\", 5),\n",
    "    (\"India\", \"USA\", 8),\n",
    "    (\"USA\", \"Canada\", 12),\n",
    "    (\"Germany\", \"France\", 6),\n",
    "    (\"Canada\", \"USA\", 15)\n",
    "]\n",
    "\n",
    "columns = [\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\", \"count\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df_data = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Repartition Data by count # Splits data into 3 partitions, ensuring that all rows with the same \"ORIGIN_COUNTRY_NAME\" are in the same partition.\n",
    "df_part = df_data.repartition(3, \"ORIGIN_COUNTRY_NAME\")\n",
    "\n",
    "\n",
    "#Sorts data within each partition in ascending order (by default) based on the \"count\" column\n",
    "df_part = df_part.sortWithinPartitions(\"count\")\n",
    "\n",
    "df_part.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96509a65-e64a-4b09-b2cb-cff145c690e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Limiting Rows (limit())\n",
    "When working with large datasets, you may want to preview or select only the top N rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b01c7a8-8e55-454c-a81c-54d53633760f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Selecting the Top 5 Rows\n",
    "df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c274b2b-3ca0-460a-857b-aaf7262ec4c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n+-----------------+-------------------+------+\n|    United States|      United States|370002|\n|    United States|             Canada|  8483|\n|           Canada|      United States|  8399|\n|    United States|             Mexico|  7187|\n|           Mexico|      United States|  7140|\n|   United Kingdom|      United States|  2025|\n+-----------------+-------------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#Selecting the Top 6 Rows in Descending Order\n",
    "df.sort(col('count').desc()).limit(6).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d084d6aa-caa3-4fa0-84fb-7b959da5ee39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Repartition and Coalesce in PySpark\n",
    "\n",
    "When working with big data in Spark, it's important to control how the data is distributed across different partitions. Partitions are chunks of data that Spark processes in parallel. Spark provides two key functions for managing partitions:\n",
    "\n",
    "- repartition() – Increases or decreases the number of partitions by performing a full shuffle.\n",
    "- coalesce() – Decreases the number of partitions without a full shuffle, making it more efficient in some cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a6d94a2-3d49-4137-b973-a7b50e3388d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[46]: 5"
     ]
    }
   ],
   "source": [
    "#Checking the Number of Partitions currently- \n",
    "df.rdd.getNumPartitions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78eba1bd-2516-4430-8785-944c19190c4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Changing the Number of Partitions- This will create 8 partitions and shuffle the data\n",
    "df = df.repartition(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "449beed9-14e2-4108-bbd5-f082c2714604",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Ireland|  344|\n|    United States|          Singapore|    1|\n|    United States|   Marshall Islands|   39|\n|    United States|             Angola|   13|\n|    United States|           Portugal|  134|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Repartition Based on a Column- If you frequently filter by a specific column, repartitioning by that column can be useful\n",
    "df = df.repartition(5, col(\"ORIGIN_COUNTRY_NAME\"))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39812e31-4a1a-42fd-9465-915fded427d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[43]: 5"
     ]
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "679e1444-c3a1-49fd-9ed1-2a39e1064c4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "coalesce() – Reduce Partitions Without Full Shuffle . It combines existing partitions instead of redistributing all data\n",
    "- If you need to reduce partitions, use coalesce(), as it is cheaper than repartition().\n",
    "- If you need even distribution of data, use repartition() (but it causes a full shuffle).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7b1977e-24b6-4331-b244-8be6ad7fb178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)\n",
    "\n",
    "#First, the data is repartitioned into 5 partitions based on DEST_COUNTRY_NAME.\n",
    "#Then, it is merged into 2 partitions without shuffling all data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45551310-4fff-4e2c-9c25-195d2c0edf00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Collecting Data to the Driver- Spark runs on a cluster, meaning data is distributed across multiple machines.\n",
    "Sometimes, you need to bring data to the driver machine for further processing\n",
    "\n",
    "Different Methods to Collect Data to the Driver\n",
    "- collect() – Retrieves all rows from the DataFrame to the driver.\n",
    "- take(n) – Fetches the first n rows.\n",
    "- show(n, truncate) – Displays n rows in a readable format.\n",
    "- toLocalIterator() – Collects partitions one by one instead of all at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aacb402-f097-4fb4-a603-e2ac17be74c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[55]: [Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1),\n Row(DEST_COUNTRY_NAME='Bolivia', ORIGIN_COUNTRY_NAME='United States', count=30),\n Row(DEST_COUNTRY_NAME='Algeria', ORIGIN_COUNTRY_NAME='United States', count=4),\n Row(DEST_COUNTRY_NAME='Turks and Caicos Islands', ORIGIN_COUNTRY_NAME='United States', count=230),\n Row(DEST_COUNTRY_NAME='Pakistan', ORIGIN_COUNTRY_NAME='United States', count=12),\n Row(DEST_COUNTRY_NAME='Marshall Islands', ORIGIN_COUNTRY_NAME='United States', count=42),\n Row(DEST_COUNTRY_NAME='Suriname', ORIGIN_COUNTRY_NAME='United States', count=1),\n Row(DEST_COUNTRY_NAME='Panama', ORIGIN_COUNTRY_NAME='United States', count=510),\n Row(DEST_COUNTRY_NAME='New Zealand', ORIGIN_COUNTRY_NAME='United States', count=111),\n Row(DEST_COUNTRY_NAME='Liberia', ORIGIN_COUNTRY_NAME='United States', count=2),\n Row(DEST_COUNTRY_NAME='Ireland', ORIGIN_COUNTRY_NAME='United States', count=335),\n Row(DEST_COUNTRY_NAME='Zambia', ORIGIN_COUNTRY_NAME='United States', count=1),\n Row(DEST_COUNTRY_NAME='Malaysia', ORIGIN_COUNTRY_NAME='United States', count=2),\n Row(DEST_COUNTRY_NAME='Japan', ORIGIN_COUNTRY_NAME='United States', count=1548),\n Row(DEST_COUNTRY_NAME='French Polynesia', ORIGIN_COUNTRY_NAME='United States', count=43),\n Row(DEST_COUNTRY_NAME='Singapore', ORIGIN_COUNTRY_NAME='United States', count=3),\n Row(DEST_COUNTRY_NAME='Denmark', ORIGIN_COUNTRY_NAME='United States', count=153),\n Row(DEST_COUNTRY_NAME='Spain', ORIGIN_COUNTRY_NAME='United States', count=420),\n Row(DEST_COUNTRY_NAME='Bermuda', ORIGIN_COUNTRY_NAME='United States', count=183),\n Row(DEST_COUNTRY_NAME='Kiribati', ORIGIN_COUNTRY_NAME='United States', count=26),\n Row(DEST_COUNTRY_NAME='Czech Republic', ORIGIN_COUNTRY_NAME='United States', count=13),\n Row(DEST_COUNTRY_NAME='Belgium', ORIGIN_COUNTRY_NAME='United States', count=259),\n Row(DEST_COUNTRY_NAME='Cape Verde', ORIGIN_COUNTRY_NAME='United States', count=20),\n Row(DEST_COUNTRY_NAME='Saint Lucia', ORIGIN_COUNTRY_NAME='United States', count=123),\n Row(DEST_COUNTRY_NAME='Tunisia', ORIGIN_COUNTRY_NAME='United States', count=3),\n Row(DEST_COUNTRY_NAME='Niger', ORIGIN_COUNTRY_NAME='United States', count=2),\n Row(DEST_COUNTRY_NAME='United Kingdom', ORIGIN_COUNTRY_NAME='United States', count=2025),\n Row(DEST_COUNTRY_NAME='Portugal', ORIGIN_COUNTRY_NAME='United States', count=127),\n Row(DEST_COUNTRY_NAME='Indonesia', ORIGIN_COUNTRY_NAME='United States', count=1),\n Row(DEST_COUNTRY_NAME='Austria', ORIGIN_COUNTRY_NAME='United States', count=62),\n Row(DEST_COUNTRY_NAME='Angola', ORIGIN_COUNTRY_NAME='United States', count=15),\n Row(DEST_COUNTRY_NAME='France', ORIGIN_COUNTRY_NAME='United States', count=935),\n Row(DEST_COUNTRY_NAME='Nigeria', ORIGIN_COUNTRY_NAME='United States', count=59),\n Row(DEST_COUNTRY_NAME='Uruguay', ORIGIN_COUNTRY_NAME='United States', count=43),\n Row(DEST_COUNTRY_NAME='Bonaire, Sint Eustatius, and Saba', ORIGIN_COUNTRY_NAME='United States', count=58),\n Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n Row(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count=588),\n Row(DEST_COUNTRY_NAME='Saint Vincent and the Grenadines', ORIGIN_COUNTRY_NAME='United States', count=1),\n Row(DEST_COUNTRY_NAME='Honduras', ORIGIN_COUNTRY_NAME='United States', count=362),\n Row(DEST_COUNTRY_NAME='The Bahamas', ORIGIN_COUNTRY_NAME='United States', count=955),\n Row(DEST_COUNTRY_NAME='El Salvador', ORIGIN_COUNTRY_NAME='United States', count=561),\n Row(DEST_COUNTRY_NAME='Hong Kong', ORIGIN_COUNTRY_NAME='United States', count=332),\n Row(DEST_COUNTRY_NAME='Mexico', ORIGIN_COUNTRY_NAME='United States', count=7140),\n Row(DEST_COUNTRY_NAME='Azerbaijan', ORIGIN_COUNTRY_NAME='United States', count=21),\n Row(DEST_COUNTRY_NAME='Barbados', ORIGIN_COUNTRY_NAME='United States', count=154),\n Row(DEST_COUNTRY_NAME='Fiji', ORIGIN_COUNTRY_NAME='United States', count=24),\n Row(DEST_COUNTRY_NAME='Canada', ORIGIN_COUNTRY_NAME='United States', count=8399),\n Row(DEST_COUNTRY_NAME='French Guiana', ORIGIN_COUNTRY_NAME='United States', count=5),\n Row(DEST_COUNTRY_NAME='Cayman Islands', ORIGIN_COUNTRY_NAME='United States', count=314),\n Row(DEST_COUNTRY_NAME=\"Cote d'Ivoire\", ORIGIN_COUNTRY_NAME='United States', count=1),\n Row(DEST_COUNTRY_NAME='Papua New Guinea', ORIGIN_COUNTRY_NAME='United States', count=3),\n Row(DEST_COUNTRY_NAME='Iraq', ORIGIN_COUNTRY_NAME='United States', count=1),\n Row(DEST_COUNTRY_NAME='Cuba', ORIGIN_COUNTRY_NAME='United States', count=466),\n Row(DEST_COUNTRY_NAME='Bahrain', ORIGIN_COUNTRY_NAME='United States', count=19),\n Row(DEST_COUNTRY_NAME='Nicaragua', ORIGIN_COUNTRY_NAME='United States', count=179),\n Row(DEST_COUNTRY_NAME='Jordan', ORIGIN_COUNTRY_NAME='United States', count=44),\n Row(DEST_COUNTRY_NAME='Ghana', ORIGIN_COUNTRY_NAME='United States', count=18),\n Row(DEST_COUNTRY_NAME='Australia', ORIGIN_COUNTRY_NAME='United States', count=329),\n Row(DEST_COUNTRY_NAME='Cook Islands', ORIGIN_COUNTRY_NAME='United States', count=13),\n Row(DEST_COUNTRY_NAME='Greece', ORIGIN_COUNTRY_NAME='United States', count=30),\n Row(DEST_COUNTRY_NAME='Guyana', ORIGIN_COUNTRY_NAME='United States', count=64),\n Row(DEST_COUNTRY_NAME='Malta', ORIGIN_COUNTRY_NAME='United States', count=1),\n Row(DEST_COUNTRY_NAME='Anguilla', ORIGIN_COUNTRY_NAME='United States', count=41),\n Row(DEST_COUNTRY_NAME='Italy', ORIGIN_COUNTRY_NAME='United States', count=382),\n Row(DEST_COUNTRY_NAME='Sint Maarten', ORIGIN_COUNTRY_NAME='United States', count=325),\n Row(DEST_COUNTRY_NAME='Trinidad and Tobago', ORIGIN_COUNTRY_NAME='United States', count=211),\n Row(DEST_COUNTRY_NAME='Colombia', ORIGIN_COUNTRY_NAME='United States', count=873),\n Row(DEST_COUNTRY_NAME='Thailand', ORIGIN_COUNTRY_NAME='United States', count=3),\n Row(DEST_COUNTRY_NAME='Antigua and Barbuda', ORIGIN_COUNTRY_NAME='United States', count=126),\n Row(DEST_COUNTRY_NAME='Ethiopia', ORIGIN_COUNTRY_NAME='United States', count=13),\n Row(DEST_COUNTRY_NAME='Germany', ORIGIN_COUNTRY_NAME='United States', count=1468),\n Row(DEST_COUNTRY_NAME='Saint Kitts and Nevis', ORIGIN_COUNTRY_NAME='United States', count=139),\n Row(DEST_COUNTRY_NAME='Kuwait', ORIGIN_COUNTRY_NAME='United States', count=32),\n Row(DEST_COUNTRY_NAME='Aruba', ORIGIN_COUNTRY_NAME='United States', count=346),\n Row(DEST_COUNTRY_NAME='Brazil', ORIGIN_COUNTRY_NAME='United States', count=853),\n Row(DEST_COUNTRY_NAME='Netherlands', ORIGIN_COUNTRY_NAME='United States', count=776),\n Row(DEST_COUNTRY_NAME='Grenada', ORIGIN_COUNTRY_NAME='United States', count=53),\n Row(DEST_COUNTRY_NAME='Russia', ORIGIN_COUNTRY_NAME='United States', count=176),\n Row(DEST_COUNTRY_NAME='Paraguay', ORIGIN_COUNTRY_NAME='United States', count=60),\n Row(DEST_COUNTRY_NAME='Kosovo', ORIGIN_COUNTRY_NAME='United States', count=1),\n Row(DEST_COUNTRY_NAME='Turkey', ORIGIN_COUNTRY_NAME='United States', count=138),\n Row(DEST_COUNTRY_NAME='Romania', ORIGIN_COUNTRY_NAME='United States', count=14),\n Row(DEST_COUNTRY_NAME='Belize', ORIGIN_COUNTRY_NAME='United States', count=188),\n Row(DEST_COUNTRY_NAME='South Korea', ORIGIN_COUNTRY_NAME='United States', count=1048),\n Row(DEST_COUNTRY_NAME='Greenland', ORIGIN_COUNTRY_NAME='United States', count=2),\n Row(DEST_COUNTRY_NAME='Chile', ORIGIN_COUNTRY_NAME='United States', count=174),\n Row(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count=40),\n Row(DEST_COUNTRY_NAME='Latvia', ORIGIN_COUNTRY_NAME='United States', count=19),\n Row(DEST_COUNTRY_NAME='Ecuador', ORIGIN_COUNTRY_NAME='United States', count=268),\n Row(DEST_COUNTRY_NAME='Venezuela', ORIGIN_COUNTRY_NAME='United States', count=290),\n Row(DEST_COUNTRY_NAME='Hungary', ORIGIN_COUNTRY_NAME='United States', count=2),\n Row(DEST_COUNTRY_NAME='Martinique', ORIGIN_COUNTRY_NAME='United States', count=44),\n Row(DEST_COUNTRY_NAME='Croatia', ORIGIN_COUNTRY_NAME='United States', count=2),\n Row(DEST_COUNTRY_NAME='Cyprus', ORIGIN_COUNTRY_NAME='United States', count=1),\n Row(DEST_COUNTRY_NAME='Qatar', ORIGIN_COUNTRY_NAME='United States', count=108),\n Row(DEST_COUNTRY_NAME='Taiwan', ORIGIN_COUNTRY_NAME='United States', count=266),\n Row(DEST_COUNTRY_NAME='Haiti', ORIGIN_COUNTRY_NAME='United States', count=226),\n Row(DEST_COUNTRY_NAME='Federated States of Micronesia', ORIGIN_COUNTRY_NAME='United States', count=69),\n Row(DEST_COUNTRY_NAME='Jamaica', ORIGIN_COUNTRY_NAME='United States', count=666),\n Row(DEST_COUNTRY_NAME='Dominican Republic', ORIGIN_COUNTRY_NAME='United States', count=1353),\n Row(DEST_COUNTRY_NAME='Finland', ORIGIN_COUNTRY_NAME='United States', count=26),\n Row(DEST_COUNTRY_NAME='British Virgin Islands', ORIGIN_COUNTRY_NAME='United States', count=107),\n Row(DEST_COUNTRY_NAME='Peru', ORIGIN_COUNTRY_NAME='United States', count=279),\n Row(DEST_COUNTRY_NAME='Saudi Arabia', ORIGIN_COUNTRY_NAME='United States', count=83),\n Row(DEST_COUNTRY_NAME='Philippines', ORIGIN_COUNTRY_NAME='United States', count=134),\n Row(DEST_COUNTRY_NAME='Ukraine', ORIGIN_COUNTRY_NAME='United States', count=14),\n Row(DEST_COUNTRY_NAME='Guatemala', ORIGIN_COUNTRY_NAME='United States', count=397),\n Row(DEST_COUNTRY_NAME='Dominica', ORIGIN_COUNTRY_NAME='United States', count=20),\n Row(DEST_COUNTRY_NAME='Guadeloupe', ORIGIN_COUNTRY_NAME='United States', count=56),\n Row(DEST_COUNTRY_NAME='Bulgaria', ORIGIN_COUNTRY_NAME='United States', count=3),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count=62),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Sint Maarten', count=325),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Marshall Islands', count=39),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Paraguay', count=6),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Gibraltar', count=1),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Federated States of Micronesia', count=69),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Russia', count=161),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Netherlands', count=660),\n Row(DEST_COUNTRY_NAME='Iceland', ORIGIN_COUNTRY_NAME='United States', count=181),\n Row(DEST_COUNTRY_NAME='Luxembourg', ORIGIN_COUNTRY_NAME='United States', count=155),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Senegal', count=42),\n Row(DEST_COUNTRY_NAME='Samoa', ORIGIN_COUNTRY_NAME='United States', count=25),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Angola', count=13),\n Row(DEST_COUNTRY_NAME='Switzerland', ORIGIN_COUNTRY_NAME='United States', count=294),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Anguilla', count=38),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ecuador', count=300),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Cyprus', count=1),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Portugal', count=134),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Costa Rica', count=608),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Guatemala', count=318),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Suriname', count=34),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Cape Verde', count=14),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Jamaica', count=712),\n Row(DEST_COUNTRY_NAME='Norway', ORIGIN_COUNTRY_NAME='United States', count=121),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Malaysia', count=3),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Morocco', count=19),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Samoa', count=25),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Palau', count=31),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Venezuela', count=246),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Chile', count=185),\n Row(DEST_COUNTRY_NAME='Morocco', ORIGIN_COUNTRY_NAME='United States', count=15),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Finland', count=28),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Greece', count=23),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='The Bahamas', count=986),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Hong Kong', count=414),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='China', count=920),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Vietnam', count=2),\n Row(DEST_COUNTRY_NAME='Burkina Faso', ORIGIN_COUNTRY_NAME='United States', count=1),\n Row(DEST_COUNTRY_NAME='Sweden', ORIGIN_COUNTRY_NAME='United States', count=118),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Kuwait', count=28),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Dominican Republic', count=1420),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Egypt', count=12),\n Row(DEST_COUNTRY_NAME='Israel', ORIGIN_COUNTRY_NAME='United States', count=134),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='United States', count=370002),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Luxembourg', count=134),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Poland', count=33),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Saint Barthelemy', count=41),\n Row(DEST_COUNTRY_NAME='Saint Barthelemy', ORIGIN_COUNTRY_NAME='United States', count=39),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Turkey', count=129),\n Row(DEST_COUNTRY_NAME='Djibouti', ORIGIN_COUNTRY_NAME='United States', count=1),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Azerbaijan', count=21),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Estonia', count=1),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='South Korea', count=827),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='El Salvador', count=508),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Hungary', count=3),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ethiopia', count=12),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Panama', count=465),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Aruba', count=342),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Thailand', count=4),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Turks and Caicos Islands', count=236),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Pakistan', count=12),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Honduras', count=407),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Liberia', count=2),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Malta', count=2),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Lithuania', count=1),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Guadeloupe', count=59),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ukraine', count=13),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='France', count=952),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Norway', count=115),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Kiribati', count=35),\n Row(DEST_COUNTRY_NAME='India', ORIGIN_COUNTRY_NAME='United States', count=61),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Germany', count=1336),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='New Zealand', count=74),\n Row(DEST_COUNTRY_NAME='United Arab Emirates', ORIGIN_COUNTRY_NAME='United States', count=320),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Mexico', count=7187),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Sweden', count=119),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Martinique', count=43),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='United Arab Emirates', count=313),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Bulgaria', count=1),\n Row(DEST_COUNTRY_NAME='China', ORIGIN_COUNTRY_NAME='United States', count=772),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Nicaragua', count=201),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Philippines', count=126),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Georgia', count=1),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Belgium', count=228),\n Row(DEST_COUNTRY_NAME='Argentina', ORIGIN_COUNTRY_NAME='United States', count=180),\n Row(DEST_COUNTRY_NAME='South Africa', ORIGIN_COUNTRY_NAME='United States', count=36),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Iceland', count=202),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Argentina', count=141),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Nigeria', count=50),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Austria', count=63),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Bonaire, Sint Eustatius, and Saba', count=59),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Israel', count=127),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Saint Lucia', count=136),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Bahrain', count=1),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='British Virgin Islands', count=80),\n Row(DEST_COUNTRY_NAME='Curacao', ORIGIN_COUNTRY_NAME='United States', count=90),\n Row(DEST_COUNTRY_NAME='Georgia', ORIGIN_COUNTRY_NAME='United States', count=2),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Denmark', count=152),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Guyana', count=63),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Papua New Guinea', count=1),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Saudi Arabia', count=70),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Curacao', count=83),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Taiwan', count=235),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='South Africa', count=40),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Greenland', count=4),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Spain', count=442),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Italy', count=438),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Switzerland', count=305),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Japan', count=1496),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Brazil', count=619),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Peru', count=337),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Belize', count=193),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='United Kingdom', count=1970),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ghana', count=20),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Indonesia', count=2),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Fiji', count=25),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Canada', count=8483),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Antigua and Barbuda', count=117),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='French Polynesia', count=40),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Latvia', count=15),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Dominica', count=27),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Czech Republic', count=12),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Australia', count=258),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Cook Islands', count=13),\n Row(DEST_COUNTRY_NAME='Palau', ORIGIN_COUNTRY_NAME='United States', count=30),\n Row(DEST_COUNTRY_NAME='New Caledonia', ORIGIN_COUNTRY_NAME='United States', count=1),\n Row(DEST_COUNTRY_NAME='Poland', ORIGIN_COUNTRY_NAME='United States', count=32),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Uruguay', count=13),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Bermuda', count=193),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Cuba', count=478),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Montenegro', count=1),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Colombia', count=867),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Barbados', count=130),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Qatar', count=109),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Cayman Islands', count=310),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Jordan', count=44),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Namibia', count=1),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Trinidad and Tobago', count=217),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Bolivia', count=13),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Saint Kitts and Nevis', count=145),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Haiti', count=225)]"
     ]
    }
   ],
   "source": [
    "# collect() – Fetch All Data\n",
    "df.collect()\n",
    "#Retrieves all rows to the driver.\n",
    "#WARNING: If the dataset is too large, this can crash the driver.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed60667a-3ea6-4e4c-8deb-4c3554290b3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[56]: [Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1),\n Row(DEST_COUNTRY_NAME='Bolivia', ORIGIN_COUNTRY_NAME='United States', count=30),\n Row(DEST_COUNTRY_NAME='Algeria', ORIGIN_COUNTRY_NAME='United States', count=4),\n Row(DEST_COUNTRY_NAME='Turks and Caicos Islands', ORIGIN_COUNTRY_NAME='United States', count=230),\n Row(DEST_COUNTRY_NAME='Pakistan', ORIGIN_COUNTRY_NAME='United States', count=12)]"
     ]
    }
   ],
   "source": [
    "#take(n) – Fetch First N Rows\n",
    "df.take(5)\n",
    "#Fetches only the first 5 rows.\n",
    "#Unlike collect(), this does not retrieve the entire DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15febef3-9eb8-489d-83c7-ad3b07db0cfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-------------------+-----+\n|DEST_COUNTRY_NAME       |ORIGIN_COUNTRY_NAME|count|\n+------------------------+-------------------+-----+\n|Moldova                 |United States      |1    |\n|Bolivia                 |United States      |30   |\n|Algeria                 |United States      |4    |\n|Turks and Caicos Islands|United States      |230  |\n|Pakistan                |United States      |12   |\n+------------------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# show(n, truncate) – Pretty Print Data\n",
    "df.show(5, False)\n",
    "#Displays the first 5 rows.\n",
    "#False ensures that long text values are not truncated.\n",
    "#This fetches partitions one by one instead of all at once.\n",
    "#Useful for very large datasets because it avoids crashing the driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32c44296-757d-4f17-b08f-a8bdc2091456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)\nRow(DEST_COUNTRY_NAME='Bolivia', ORIGIN_COUNTRY_NAME='United States', count=30)\nRow(DEST_COUNTRY_NAME='Algeria', ORIGIN_COUNTRY_NAME='United States', count=4)\nRow(DEST_COUNTRY_NAME='Turks and Caicos Islands', ORIGIN_COUNTRY_NAME='United States', count=230)\nRow(DEST_COUNTRY_NAME='Pakistan', ORIGIN_COUNTRY_NAME='United States', count=12)\nRow(DEST_COUNTRY_NAME='Marshall Islands', ORIGIN_COUNTRY_NAME='United States', count=42)\nRow(DEST_COUNTRY_NAME='Suriname', ORIGIN_COUNTRY_NAME='United States', count=1)\nRow(DEST_COUNTRY_NAME='Panama', ORIGIN_COUNTRY_NAME='United States', count=510)\nRow(DEST_COUNTRY_NAME='New Zealand', ORIGIN_COUNTRY_NAME='United States', count=111)\nRow(DEST_COUNTRY_NAME='Liberia', ORIGIN_COUNTRY_NAME='United States', count=2)\nRow(DEST_COUNTRY_NAME='Ireland', ORIGIN_COUNTRY_NAME='United States', count=335)\nRow(DEST_COUNTRY_NAME='Zambia', ORIGIN_COUNTRY_NAME='United States', count=1)\nRow(DEST_COUNTRY_NAME='Malaysia', ORIGIN_COUNTRY_NAME='United States', count=2)\nRow(DEST_COUNTRY_NAME='Japan', ORIGIN_COUNTRY_NAME='United States', count=1548)\nRow(DEST_COUNTRY_NAME='French Polynesia', ORIGIN_COUNTRY_NAME='United States', count=43)\nRow(DEST_COUNTRY_NAME='Singapore', ORIGIN_COUNTRY_NAME='United States', count=3)\nRow(DEST_COUNTRY_NAME='Denmark', ORIGIN_COUNTRY_NAME='United States', count=153)\nRow(DEST_COUNTRY_NAME='Spain', ORIGIN_COUNTRY_NAME='United States', count=420)\nRow(DEST_COUNTRY_NAME='Bermuda', ORIGIN_COUNTRY_NAME='United States', count=183)\nRow(DEST_COUNTRY_NAME='Kiribati', ORIGIN_COUNTRY_NAME='United States', count=26)\nRow(DEST_COUNTRY_NAME='Czech Republic', ORIGIN_COUNTRY_NAME='United States', count=13)\nRow(DEST_COUNTRY_NAME='Belgium', ORIGIN_COUNTRY_NAME='United States', count=259)\nRow(DEST_COUNTRY_NAME='Cape Verde', ORIGIN_COUNTRY_NAME='United States', count=20)\nRow(DEST_COUNTRY_NAME='Saint Lucia', ORIGIN_COUNTRY_NAME='United States', count=123)\nRow(DEST_COUNTRY_NAME='Tunisia', ORIGIN_COUNTRY_NAME='United States', count=3)\nRow(DEST_COUNTRY_NAME='Niger', ORIGIN_COUNTRY_NAME='United States', count=2)\nRow(DEST_COUNTRY_NAME='United Kingdom', ORIGIN_COUNTRY_NAME='United States', count=2025)\nRow(DEST_COUNTRY_NAME='Portugal', ORIGIN_COUNTRY_NAME='United States', count=127)\nRow(DEST_COUNTRY_NAME='Indonesia', ORIGIN_COUNTRY_NAME='United States', count=1)\nRow(DEST_COUNTRY_NAME='Austria', ORIGIN_COUNTRY_NAME='United States', count=62)\nRow(DEST_COUNTRY_NAME='Angola', ORIGIN_COUNTRY_NAME='United States', count=15)\nRow(DEST_COUNTRY_NAME='France', ORIGIN_COUNTRY_NAME='United States', count=935)\nRow(DEST_COUNTRY_NAME='Nigeria', ORIGIN_COUNTRY_NAME='United States', count=59)\nRow(DEST_COUNTRY_NAME='Uruguay', ORIGIN_COUNTRY_NAME='United States', count=43)\nRow(DEST_COUNTRY_NAME='Bonaire, Sint Eustatius, and Saba', ORIGIN_COUNTRY_NAME='United States', count=58)\nRow(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15)\nRow(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count=588)\nRow(DEST_COUNTRY_NAME='Saint Vincent and the Grenadines', ORIGIN_COUNTRY_NAME='United States', count=1)\nRow(DEST_COUNTRY_NAME='Honduras', ORIGIN_COUNTRY_NAME='United States', count=362)\nRow(DEST_COUNTRY_NAME='The Bahamas', ORIGIN_COUNTRY_NAME='United States', count=955)\nRow(DEST_COUNTRY_NAME='El Salvador', ORIGIN_COUNTRY_NAME='United States', count=561)\nRow(DEST_COUNTRY_NAME='Hong Kong', ORIGIN_COUNTRY_NAME='United States', count=332)\nRow(DEST_COUNTRY_NAME='Mexico', ORIGIN_COUNTRY_NAME='United States', count=7140)\nRow(DEST_COUNTRY_NAME='Azerbaijan', ORIGIN_COUNTRY_NAME='United States', count=21)\nRow(DEST_COUNTRY_NAME='Barbados', ORIGIN_COUNTRY_NAME='United States', count=154)\nRow(DEST_COUNTRY_NAME='Fiji', ORIGIN_COUNTRY_NAME='United States', count=24)\nRow(DEST_COUNTRY_NAME='Canada', ORIGIN_COUNTRY_NAME='United States', count=8399)\nRow(DEST_COUNTRY_NAME='French Guiana', ORIGIN_COUNTRY_NAME='United States', count=5)\nRow(DEST_COUNTRY_NAME='Cayman Islands', ORIGIN_COUNTRY_NAME='United States', count=314)\nRow(DEST_COUNTRY_NAME=\"Cote d'Ivoire\", ORIGIN_COUNTRY_NAME='United States', count=1)\nRow(DEST_COUNTRY_NAME='Papua New Guinea', ORIGIN_COUNTRY_NAME='United States', count=3)\nRow(DEST_COUNTRY_NAME='Iraq', ORIGIN_COUNTRY_NAME='United States', count=1)\nRow(DEST_COUNTRY_NAME='Cuba', ORIGIN_COUNTRY_NAME='United States', count=466)\nRow(DEST_COUNTRY_NAME='Bahrain', ORIGIN_COUNTRY_NAME='United States', count=19)\nRow(DEST_COUNTRY_NAME='Nicaragua', ORIGIN_COUNTRY_NAME='United States', count=179)\nRow(DEST_COUNTRY_NAME='Jordan', ORIGIN_COUNTRY_NAME='United States', count=44)\nRow(DEST_COUNTRY_NAME='Ghana', ORIGIN_COUNTRY_NAME='United States', count=18)\nRow(DEST_COUNTRY_NAME='Australia', ORIGIN_COUNTRY_NAME='United States', count=329)\nRow(DEST_COUNTRY_NAME='Cook Islands', ORIGIN_COUNTRY_NAME='United States', count=13)\nRow(DEST_COUNTRY_NAME='Greece', ORIGIN_COUNTRY_NAME='United States', count=30)\nRow(DEST_COUNTRY_NAME='Guyana', ORIGIN_COUNTRY_NAME='United States', count=64)\nRow(DEST_COUNTRY_NAME='Malta', ORIGIN_COUNTRY_NAME='United States', count=1)\nRow(DEST_COUNTRY_NAME='Anguilla', ORIGIN_COUNTRY_NAME='United States', count=41)\nRow(DEST_COUNTRY_NAME='Italy', ORIGIN_COUNTRY_NAME='United States', count=382)\nRow(DEST_COUNTRY_NAME='Sint Maarten', ORIGIN_COUNTRY_NAME='United States', count=325)\nRow(DEST_COUNTRY_NAME='Trinidad and Tobago', ORIGIN_COUNTRY_NAME='United States', count=211)\nRow(DEST_COUNTRY_NAME='Colombia', ORIGIN_COUNTRY_NAME='United States', count=873)\nRow(DEST_COUNTRY_NAME='Thailand', ORIGIN_COUNTRY_NAME='United States', count=3)\nRow(DEST_COUNTRY_NAME='Antigua and Barbuda', ORIGIN_COUNTRY_NAME='United States', count=126)\nRow(DEST_COUNTRY_NAME='Ethiopia', ORIGIN_COUNTRY_NAME='United States', count=13)\nRow(DEST_COUNTRY_NAME='Germany', ORIGIN_COUNTRY_NAME='United States', count=1468)\nRow(DEST_COUNTRY_NAME='Saint Kitts and Nevis', ORIGIN_COUNTRY_NAME='United States', count=139)\nRow(DEST_COUNTRY_NAME='Kuwait', ORIGIN_COUNTRY_NAME='United States', count=32)\nRow(DEST_COUNTRY_NAME='Aruba', ORIGIN_COUNTRY_NAME='United States', count=346)\nRow(DEST_COUNTRY_NAME='Brazil', ORIGIN_COUNTRY_NAME='United States', count=853)\nRow(DEST_COUNTRY_NAME='Netherlands', ORIGIN_COUNTRY_NAME='United States', count=776)\nRow(DEST_COUNTRY_NAME='Grenada', ORIGIN_COUNTRY_NAME='United States', count=53)\nRow(DEST_COUNTRY_NAME='Russia', ORIGIN_COUNTRY_NAME='United States', count=176)\nRow(DEST_COUNTRY_NAME='Paraguay', ORIGIN_COUNTRY_NAME='United States', count=60)\nRow(DEST_COUNTRY_NAME='Kosovo', ORIGIN_COUNTRY_NAME='United States', count=1)\nRow(DEST_COUNTRY_NAME='Turkey', ORIGIN_COUNTRY_NAME='United States', count=138)\nRow(DEST_COUNTRY_NAME='Romania', ORIGIN_COUNTRY_NAME='United States', count=14)\nRow(DEST_COUNTRY_NAME='Belize', ORIGIN_COUNTRY_NAME='United States', count=188)\nRow(DEST_COUNTRY_NAME='South Korea', ORIGIN_COUNTRY_NAME='United States', count=1048)\nRow(DEST_COUNTRY_NAME='Greenland', ORIGIN_COUNTRY_NAME='United States', count=2)\nRow(DEST_COUNTRY_NAME='Chile', ORIGIN_COUNTRY_NAME='United States', count=174)\nRow(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count=40)\nRow(DEST_COUNTRY_NAME='Latvia', ORIGIN_COUNTRY_NAME='United States', count=19)\nRow(DEST_COUNTRY_NAME='Ecuador', ORIGIN_COUNTRY_NAME='United States', count=268)\nRow(DEST_COUNTRY_NAME='Venezuela', ORIGIN_COUNTRY_NAME='United States', count=290)\nRow(DEST_COUNTRY_NAME='Hungary', ORIGIN_COUNTRY_NAME='United States', count=2)\nRow(DEST_COUNTRY_NAME='Martinique', ORIGIN_COUNTRY_NAME='United States', count=44)\nRow(DEST_COUNTRY_NAME='Croatia', ORIGIN_COUNTRY_NAME='United States', count=2)\nRow(DEST_COUNTRY_NAME='Cyprus', ORIGIN_COUNTRY_NAME='United States', count=1)\nRow(DEST_COUNTRY_NAME='Qatar', ORIGIN_COUNTRY_NAME='United States', count=108)\nRow(DEST_COUNTRY_NAME='Taiwan', ORIGIN_COUNTRY_NAME='United States', count=266)\nRow(DEST_COUNTRY_NAME='Haiti', ORIGIN_COUNTRY_NAME='United States', count=226)\nRow(DEST_COUNTRY_NAME='Federated States of Micronesia', ORIGIN_COUNTRY_NAME='United States', count=69)\nRow(DEST_COUNTRY_NAME='Jamaica', ORIGIN_COUNTRY_NAME='United States', count=666)\nRow(DEST_COUNTRY_NAME='Dominican Republic', ORIGIN_COUNTRY_NAME='United States', count=1353)\nRow(DEST_COUNTRY_NAME='Finland', ORIGIN_COUNTRY_NAME='United States', count=26)\nRow(DEST_COUNTRY_NAME='British Virgin Islands', ORIGIN_COUNTRY_NAME='United States', count=107)\nRow(DEST_COUNTRY_NAME='Peru', ORIGIN_COUNTRY_NAME='United States', count=279)\nRow(DEST_COUNTRY_NAME='Saudi Arabia', ORIGIN_COUNTRY_NAME='United States', count=83)\nRow(DEST_COUNTRY_NAME='Philippines', ORIGIN_COUNTRY_NAME='United States', count=134)\nRow(DEST_COUNTRY_NAME='Ukraine', ORIGIN_COUNTRY_NAME='United States', count=14)\nRow(DEST_COUNTRY_NAME='Guatemala', ORIGIN_COUNTRY_NAME='United States', count=397)\nRow(DEST_COUNTRY_NAME='Dominica', ORIGIN_COUNTRY_NAME='United States', count=20)\nRow(DEST_COUNTRY_NAME='Guadeloupe', ORIGIN_COUNTRY_NAME='United States', count=56)\nRow(DEST_COUNTRY_NAME='Bulgaria', ORIGIN_COUNTRY_NAME='United States', count=3)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count=62)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Sint Maarten', count=325)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Marshall Islands', count=39)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Paraguay', count=6)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Gibraltar', count=1)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Federated States of Micronesia', count=69)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Russia', count=161)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Netherlands', count=660)\nRow(DEST_COUNTRY_NAME='Iceland', ORIGIN_COUNTRY_NAME='United States', count=181)\nRow(DEST_COUNTRY_NAME='Luxembourg', ORIGIN_COUNTRY_NAME='United States', count=155)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Senegal', count=42)\nRow(DEST_COUNTRY_NAME='Samoa', ORIGIN_COUNTRY_NAME='United States', count=25)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Angola', count=13)\nRow(DEST_COUNTRY_NAME='Switzerland', ORIGIN_COUNTRY_NAME='United States', count=294)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Anguilla', count=38)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ecuador', count=300)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Cyprus', count=1)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Portugal', count=134)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Costa Rica', count=608)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Guatemala', count=318)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Suriname', count=34)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Cape Verde', count=14)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Jamaica', count=712)\nRow(DEST_COUNTRY_NAME='Norway', ORIGIN_COUNTRY_NAME='United States', count=121)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Malaysia', count=3)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Morocco', count=19)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Samoa', count=25)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Palau', count=31)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Venezuela', count=246)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Chile', count=185)\nRow(DEST_COUNTRY_NAME='Morocco', ORIGIN_COUNTRY_NAME='United States', count=15)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Finland', count=28)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Greece', count=23)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='The Bahamas', count=986)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Hong Kong', count=414)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='China', count=920)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Vietnam', count=2)\nRow(DEST_COUNTRY_NAME='Burkina Faso', ORIGIN_COUNTRY_NAME='United States', count=1)\nRow(DEST_COUNTRY_NAME='Sweden', ORIGIN_COUNTRY_NAME='United States', count=118)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Kuwait', count=28)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Dominican Republic', count=1420)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Egypt', count=12)\nRow(DEST_COUNTRY_NAME='Israel', ORIGIN_COUNTRY_NAME='United States', count=134)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='United States', count=370002)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Luxembourg', count=134)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Poland', count=33)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Saint Barthelemy', count=41)\nRow(DEST_COUNTRY_NAME='Saint Barthelemy', ORIGIN_COUNTRY_NAME='United States', count=39)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Turkey', count=129)\nRow(DEST_COUNTRY_NAME='Djibouti', ORIGIN_COUNTRY_NAME='United States', count=1)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Azerbaijan', count=21)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Estonia', count=1)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='South Korea', count=827)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='El Salvador', count=508)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Hungary', count=3)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ethiopia', count=12)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Panama', count=465)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Aruba', count=342)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Thailand', count=4)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Turks and Caicos Islands', count=236)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Pakistan', count=12)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Honduras', count=407)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Liberia', count=2)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Malta', count=2)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Lithuania', count=1)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Guadeloupe', count=59)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ukraine', count=13)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='France', count=952)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Norway', count=115)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Kiribati', count=35)\nRow(DEST_COUNTRY_NAME='India', ORIGIN_COUNTRY_NAME='United States', count=61)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Germany', count=1336)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='New Zealand', count=74)\nRow(DEST_COUNTRY_NAME='United Arab Emirates', ORIGIN_COUNTRY_NAME='United States', count=320)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Mexico', count=7187)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Sweden', count=119)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Martinique', count=43)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='United Arab Emirates', count=313)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Bulgaria', count=1)\nRow(DEST_COUNTRY_NAME='China', ORIGIN_COUNTRY_NAME='United States', count=772)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Nicaragua', count=201)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Philippines', count=126)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Georgia', count=1)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Belgium', count=228)\nRow(DEST_COUNTRY_NAME='Argentina', ORIGIN_COUNTRY_NAME='United States', count=180)\nRow(DEST_COUNTRY_NAME='South Africa', ORIGIN_COUNTRY_NAME='United States', count=36)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Iceland', count=202)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Argentina', count=141)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Nigeria', count=50)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Austria', count=63)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Bonaire, Sint Eustatius, and Saba', count=59)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Israel', count=127)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Saint Lucia', count=136)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Bahrain', count=1)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='British Virgin Islands', count=80)\nRow(DEST_COUNTRY_NAME='Curacao', ORIGIN_COUNTRY_NAME='United States', count=90)\nRow(DEST_COUNTRY_NAME='Georgia', ORIGIN_COUNTRY_NAME='United States', count=2)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Denmark', count=152)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Guyana', count=63)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Papua New Guinea', count=1)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Saudi Arabia', count=70)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Curacao', count=83)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Taiwan', count=235)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='South Africa', count=40)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Greenland', count=4)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Spain', count=442)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Italy', count=438)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Switzerland', count=305)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Japan', count=1496)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Brazil', count=619)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Peru', count=337)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Belize', count=193)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='United Kingdom', count=1970)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ghana', count=20)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Indonesia', count=2)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Fiji', count=25)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Canada', count=8483)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Antigua and Barbuda', count=117)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='French Polynesia', count=40)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Latvia', count=15)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Dominica', count=27)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Czech Republic', count=12)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Australia', count=258)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Cook Islands', count=13)\nRow(DEST_COUNTRY_NAME='Palau', ORIGIN_COUNTRY_NAME='United States', count=30)\nRow(DEST_COUNTRY_NAME='New Caledonia', ORIGIN_COUNTRY_NAME='United States', count=1)\nRow(DEST_COUNTRY_NAME='Poland', ORIGIN_COUNTRY_NAME='United States', count=32)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Uruguay', count=13)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Bermuda', count=193)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Cuba', count=478)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Montenegro', count=1)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Colombia', count=867)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Barbados', count=130)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Qatar', count=109)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Cayman Islands', count=310)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Jordan', count=44)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Namibia', count=1)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Trinidad and Tobago', count=217)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Bolivia', count=13)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Saint Kitts and Nevis', count=145)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Haiti', count=225)\n"
     ]
    }
   ],
   "source": [
    "# toLocalIterator() – Fetch Data Partition-by-Partition\n",
    "for row in df.toLocalIterator():\n",
    "    print(row)\n",
    "\n",
    "#This fetches partitions one by one instead of all at once.\n",
    "#Useful for very large datasets because it avoids crashing the driver better than collect but of partitions are very large it can crash the driver.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3893457758963641,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark  Definitive Guide- Level 1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
